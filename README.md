# ğŸ§  Proyecto: ExtracciÃ³n y VisualizaciÃ³n de InformaciÃ³n sobre Ciencia de Datos  

## ğŸ“‹ DescripciÃ³n General  
Este proyecto realiza un **proceso completo de Web Scraping, Limpieza de Texto, ETL y VisualizaciÃ³n de Datos**, con el objetivo de **extraer informaciÃ³n relevante sobre Ciencia de Datos desde distintas fuentes web** y **representarla visualmente mediante una nube de palabras y un grÃ¡fico de frecuencias**.  

El flujo del anÃ¡lisis sigue una estructura **ETL (Extract, Transform, Load)**:  
1. **ExtracciÃ³n** de texto desde mÃºltiples URLs (IBM, Tableau, Wikipedia).  
2. **TransformaciÃ³n** mediante tokenizaciÃ³n, limpieza lingÃ¼Ã­stica y eliminaciÃ³n de *stopwords*.  
3. **Carga y visualizaciÃ³n** final en un **reporte PDF automatizado** con dos grÃ¡ficos generados en Python.  

---

## ğŸš€ Objetivo del Proyecto  
Aprender de forma prÃ¡ctica y **autÃ³noma** cÃ³mo realizar un flujo de trabajo real de anÃ¡lisis textual en Ciencia de Datos, incluyendo:  

- Web scraping intuitivo.  
- Procesamiento y limpieza de texto.  
- GeneraciÃ³n de una nube de palabras reproducible.  
- VisualizaciÃ³n de las palabras mÃ¡s frecuentes.  
- ExportaciÃ³n automÃ¡tica a PDF para reportes profesionales.  

---
## ğŸ§¾ Estructura de Archivos  
ğŸ“‚ proyecto_wordcloud/
â”œâ”€â”€ ğŸ script_wordcloud.py â†’ CÃ³digo principal del anÃ¡lisis
â”œâ”€â”€ â˜ï¸ nube_palabras.png â†’ Nube de palabras generada
â”œâ”€â”€ ğŸ“Š grafico_frecuencia.png â†’ GrÃ¡fico de top 10 palabras
â”œâ”€â”€ ğŸ“„ reporte_visual.pdf â†’ Reporte final exportado
â””â”€â”€ ğŸ§  README.md â†’ DocumentaciÃ³n del proyecto


--

## ğŸ§° TecnologÃ­as y LibrerÃ­as Utilizadas  

| Etapa | LibrerÃ­as / Herramientas |
|-------|---------------------------|
| Web Scraping | `requests`, `BeautifulSoup` |
| Procesamiento de Texto | `re`, `nltk`, `collections.Counter` |
| VisualizaciÃ³n | `matplotlib`, `wordcloud` |
| Reporte Automatizado | `fpdf` |
| Entorno de Trabajo | Spyder (Python IDE) |

---

## ğŸ§© Flujo del Proceso  

### 1ï¸âƒ£ ExtracciÃ³n  
Se recopilan textos desde URLs de referencia en ciencia de datos e inteligencia artificial:  
- [Tableau - Data Science](https://www.tableau.com/es-es/data-insights/data-science)  
- [IBM - Data Science](https://www.ibm.com/es-es/topics/data-science)  
- [Wikipedia - Inteligencia Artificial](https://es.wikipedia.org/wiki/Inteligencia_artificial)  
- entre otras.  

El texto de cada sitio se obtiene con `BeautifulSoup` y se acumula para su posterior anÃ¡lisis.  

### 2ï¸âƒ£ TransformaciÃ³n (Limpieza y TokenizaciÃ³n)  
- TokenizaciÃ³n del texto en palabras.  
- EliminaciÃ³n de signos, nÃºmeros y *stopwords* (en espaÃ±ol e inglÃ©s).  
- Filtrado adicional de palabras irrelevantes o redundantes.  

### 3ï¸âƒ£ Carga y VisualizaciÃ³n  
- Se genera una **nube de palabras (WordCloud)** con `random_state=53` para garantizar **reproducibilidad** visual.  
- Se crea un **grÃ¡fico de barras** con las 10 palabras mÃ¡s frecuentes.  

### 4ï¸âƒ£ Reporte Automatizado (PDF)  
Ambas visualizaciones se exportan en un **reporte visual** (`reporte_visual.pdf`) que incluye:  
- TÃ­tulo con fecha del dÃ­a.  
- Imagen de la nube de palabras.  
- GrÃ¡fico de frecuencia de tÃ©rminos.  

---

## ğŸ“Š Ejemplo de Resultados  

### ğŸ©µ Nube de Palabras  
Representa los tÃ©rminos mÃ¡s frecuentes en los textos de Ciencia de Datos.  

### ğŸ©µ GrÃ¡fico de Frecuencia  
Muestra las **10 palabras mÃ¡s mencionadas**, reflejando los conceptos dominantes en las fuentes analizadas.  

### ğŸ©µ Reporte PDF  
El resultado final es un **reporte automÃ¡tico y portable** que consolida los hallazgos visuales del anÃ¡lisis.  

---

## ğŸ’¡ Aprendizajes Clave  
- Implementar **Web Scraping de forma intuitiva** para extraer informaciÃ³n Ãºtil de la web.  
- Aplicar **procesos ETL** al tratamiento de texto.  
- Generar **visualizaciones reproducibles** (controlando la aleatoriedad con `random_state`).  
- Crear **reportes visuales automatizados** para comunicar resultados.  

---

## âš™ï¸ Requisitos de EjecuciÃ³n  

Instalar las dependencias necesarias (puedes usar un entorno virtual):  

```bash
- pip install requests beautifulsoup4 matplotlib wordcloud nltk fpdf
- import nltk
nltk.download("stopwords")


